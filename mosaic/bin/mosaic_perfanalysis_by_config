#!/usr/bin/env python3
# coding: utf-8

from argparse import ArgumentParser
from configparser import ConfigParser
import json
import pandas as pd
import numpy as np
import sqlite3
import re


'''

	include => param_value
	exclude => a prendre dans la pipeline

	ordinate => Soit run_results soit params %width
	abscissa => 


	[{'Challenge'}, {'class' : 'mlp', 'depth':4}]
	
	'Challenge' | 'mlp'


			abs1 abs2 abs3
pipeline	val1 val2 val3

			ord1 ord2 ord3
pipeline	val1 val2 val3
pipeline
pipeline2


liste_abs_unique = abs1 abs2 abs3



	1 - Récupérer le SELECT que j'utilisais avant pour pourvoir récupérer les ids et les pipelines dans le même temps
		1.1 - Faire une map qui associe les pipelines avec les ids pour pouvoir utiliser les pipelines dans un second temps

	2 - Avec ces ids, je peux aller plus facilement récupérer les valeurs dans les différentes tables en fonction de si le paramètre est dans run_result ou pas

	3 - Pour chaque id qui se trouvent dans ids, je vais faire une requête pour récupérer l'élement de l'abscisse et de l'ordonnée associé à l'ID
		3.1 - Je stocke tout ça dans un dictionnaire qui aura comme clé le nom du paramètre que je viens de chercher dans la db avec un dictionnaire avec la pipeline et la valeur du paramètre pour valeur

	4 - Je fusionne les deux dictionnaires et les mets dans un dataframe

	5 - Je peux boucler sur les abscisses uniques, faire des groupby et faire des plots en fonction de ces groupby

	6 - Sauvegarder le plot comme une page d'un fichier PDF que je peux ouvrir avec FPDF par exemple


	for _id in ids_list:
		for param in ordinate
		
			# Il faut que je stocke ça dans une liste ou dans un dictionnaire de liste : {param : {pipeline : result}} --> Transformable en DF
			
			self.cur.execute("""SELECT DISTINCT p0.param_value, p1.param_value
			FROM params p0
				INNER JOIN params p1 ON p1.run_id = p0.run_id
			WHERE p0.param_name LIKE "%pipeline" AND p1.param_name LIKE "%{param}" AND p0.run_id = ?""").fetchall()




	SELECT DISTINCT p0.run_id, p1.pipeline
	FROM params p0
		INNER JOIN params p1 ON p1.run_id = p0.run_id
	WHERE p1.param_name = "%pipeline" AND (p0.param_value 



	Faire une liste de liste des élements qui se trouvent dans include_values


'''



class Perf_Analysis():
	def __init__(self, config_file, database_file):
		self.config_file = config_file
		self.database_file = database_file
		self.conf_parser = ConfigParser()
		self.conf_parser.read(config_file)
		self.scheduler()

	def _format_pipeline(self, pipe):
		pipe = json.loads(pipe)
		res = []
		for elem in pipe:
			values = []
			for key, value in elem.items():
				if (key != 'name' and key != 'type' and key != 'path_to_class'
				and key != 'class' and key != 'key'):
					values.append(str(value))
			res.append(elem['class'] + '(' + ','.join(values) + ')')
		return ' | '.join(res)

	def replace_list_config_section(self, config_section):
		for key, values in config_section.items():
			li = re.findall(r'[\w\.\/\:{}-]+', str(values))
			config_section[key] = li
		return config_section

	def crop_pipeline(self, pipeline):
		pipeline = json.loads(pipeline)
		for module in pipeline:
			for to_exclude in self.exclude:
				if to_exclude in module: 
					module.pop(to_exclude)
		return json.dumps(pipeline)
	
	def groupby_mean_std(self, data):
		aggr_dict = {}
		for col in self.ordinate:
			aggr_dict[col] = [np.mean, np.std]

		return data.groupby(self.abscissa + ['pipeline', 'label']).agg(aggr_dict)

	def get_dataframe_from_db(self):
		self.con = sqlite3.connect(self.database_file)
		self.cur = self.con.cursor()

		_select = (	'SELECT DISTINCT '
					+ ', '.join([f'p0.{ab}' for ab in self.abscissa] + [f'p0.{ordi}' for ordi in self.ordinate])
					+ ', p1.param_value')

		_where = ('WHERE p1.param_name LIKE "%pipeline" AND p2.pipeline LIKE ' + ' AND p2.pipeline LIKE '.join([f'"%{include}%"' for include in self.include]))

		data = pd.DataFrame(self.cur.execute(f'''
			{_select}
			FROM run_results p0
				INNER JOIN params p1 ON p0.run_id = p1.run_id
				INNER JOIN runs p2 ON p0.run_id = p2.run_id
			{_where}
		''').fetchall(), columns=self.abscissa + self.ordinate + ['pipeline'])

		self.con.close()
		return data

	def get_dataframe_from_db(self):
		self.con = sqlite3.connect(self.database_file)
		self.cur = self.con.cursor()

		_select = 'SELECT DISTINCT p0.run_id, p1.param_value'

		_where = ('WHERE p1.param_name LIKE "%pipeline" AND p2.pipeline LIKE ' + ' AND p2.pipeline LIKE '.join([f'"%{include}%"' for include in self.include]))

		data = pd.DataFrame(self.cur.execute(f'''
			{_select}
			FROM run_results p0
				INNER JOIN params p1 ON p0.run_id = p1.run_id
				INNER JOIN runs p2 ON p0.run_id = p2.run_id
			{_where}
		''').fetchall(), columns=['run_id', 'pipeline'])

		# Avec tous les ids, aller chercher les éléments qui se trouvent dans abscissa et ordinate dans la db
		# Si l'élément n'est pas dans liste alors j'utilise le SELECT que j'ai écris plus haut
		# Sinon, je requête directement run_result avec le bon paramètre
		# Si je le trouve nul part, alors je renvoie une erreur ? LE but étant de vouloir faire des plots et si le nom n'y est pas c'est que l'utilisateur s'est trompé


		abscissa_dict = {}
		for ab in self.abscissa:
			for _, (run_id, pipeline) in data.iterrows():
				print('[+]run_id, ', run_id, '\n[+]pipeline:', pipeline, '\n\n')
				if ab not in ['test_loss', 'train_loss', 'test_acc', 'train_acc', 'epochs', 'nb_params', 'duration(s)', 'overfit', 'trainability', 'slope']:
					param = f'%{ab}'
					val = self.cur.execute(f"""SELECT DISTINCT p0.param_value
						FROM params p0
						WHERE p0.param_name LIKE ? AND p0.run_id = ?""", (param, run_id)).fetchone()
					if val is None:
						print('ERROR')
						exit(1)

					abscissa_dict[run_id] = {pipeline : val}


		self.con.close()
		exit()
		return data

	def make_a_plot(self, config_section, plot_name):
		self.abscissa = config_section['abscissa']
		self.ordinate = config_section['ordinate']
		self.include = config_section['include']
		self.exclude = config_section['exclude']

		data = self.get_dataframe_from_db()

		data['pipeline'] = data['pipeline'].apply(self.crop_pipeline)
		df = data['pipeline'].apply(lambda row : self._format_pipeline(row))
		data['label'] = df
		data = self.groupby_mean_std(data)
		print(data)
		

	def scheduler(self):
		for plot_name in self.conf_parser.sections():
			if plot_name != 'global':
				section = dict(self.conf_parser[plot_name])
				section = self.replace_list_config_section(section)
				self.make_a_plot(section, plot_name)




if __name__ == '__main__':
	arg_parser = ArgumentParser()
	arg_parser.add_argument('config_file', type=str)
	arg_parser.add_argument('database_file', type=str)
	args = vars(arg_parser.parse_args())

	Perf_Analysis(args['config_file'], args['database_file'])